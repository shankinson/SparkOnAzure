#cloud-config
package_upgrade: true
packages:
  - openjdk-8-jdk
runcmd:
  - 'wget https://reportresources.blob.core.windows.net/public/spark-2.4.3-bin-without-hadoop.tgz -O /home/spark/spark-2.4.3-bin-without-hadoop.tgz'
  - tar -xzf /home/spark/spark-2.4.3-bin-without-hadoop.tgz -C /home/spark/
  - rm /home/spark/spark-2.4.3-bin-without-hadoop.tgz
  - 'wget https://reportresources.blob.core.windows.net/public/hadoop-3.2.0.tar.gz -O /home/spark/hadoop-3.2.0.tar.gz'
  - tar -xzf /home/spark/hadoop-3.2.0.tar.gz -C /home/spark/
  - rm /home/spark/hadoop-3.2.0.tar.gz
  - cp /home/spark/hadoop-3.2.0/share/hadoop/tools/lib/azure-storage-7.0.0.jar /home/spark/spark-2.4.3-bin-without-hadoop/jars/azure-storage-7.0.0.jar
  - cp /home/spark/hadoop-3.2.0/share/hadoop/tools/lib/azure-keyvault-core-1.0.0.jar /home/spark/spark-2.4.3-bin-without-hadoop/jars/azure-keyvault-core-1.0.0.jar
  - cp /home/spark/hadoop-3.2.0/share/hadoop/tools/lib/azure-data-lake-store-sdk-2.2.9.jar /home/spark/spark-2.4.3-bin-without-hadoop/jars/azure-data-lake-store-sdk-2.2.9.jar
  - cp /home/spark/hadoop-3.2.0/share/hadoop/tools/lib/hadoop-azure-3.2.0.jar /home/spark/spark-2.4.3-bin-without-hadoop/jars/hadoop-azure-3.2.0.jar
  - cp /home/spark/hadoop-3.2.0/share/hadoop/tools/lib/hadoop-azure-datalake-3.2.0.jar /home/spark/spark-2.4.3-bin-without-hadoop/jars/hadoop-azure-datalake-3.2.0.jar
  - cp /home/spark/hadoop-3.2.0/share/hadoop/tools/lib/wildfly-openssl-1.0.4.Final.jar /home/spark/spark-2.4.3-bin-without-hadoop/jars/wildfly-openssl-1.0.4.Final.jar
  - echo "SPARK_LOCAL_DIRS=/mnt/spark" > /home/spark/spark-2.4.3-bin-without-hadoop/conf/spark-env.sh
  - echo 'JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64' >> /home/spark/spark-2.4.3-bin-without-hadoop/conf/spark-env.sh
  - echo 'SPARK_DIST_CLASSPATH=$(/home/spark/hadoop-3.2.0/bin/hadoop classpath)' >> /home/spark/spark-2.4.3-bin-without-hadoop/conf/spark-env.sh
  - chmod 755 /home/spark/spark-2.4.3-bin-without-hadoop/conf/spark-env.sh
  - chown -R spark.spark /home/spark/spark-2.4.3-bin-without-hadoop
  - 'echo "<?xml version=\"1.0\" encoding=\"UTF-8\"?>" > /home/spark/hadoop-3.2.0/etc/hadoop/core-site.xml'
  - 'echo "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>" >> /home/spark/hadoop-3.2.0/etc/hadoop/core-site.xml'
  - 'echo "<configuration>" >> /home/spark/hadoop-3.2.0/etc/hadoop/core-site.xml'
  - 'echo "  <property>" >> /home/spark/hadoop-3.2.0/etc/hadoop/core-site.xml'
  - 'echo "    <name>fs.defaultFS</name>" >> /home/spark/hadoop-3.2.0/etc/hadoop/core-site.xml'
  - 'echo "    <value>hdfs://172.16.1.4</value>" >> /home/spark/hadoop-3.2.0/etc/hadoop/core-site.xml'
  - 'echo "  </property>" >> /home/spark/hadoop-3.2.0/etc/hadoop/core-site.xml'
  - 'echo "<property>" >> /home/spark/hadoop-3.2.0/etc/hadoop/core-site.xml'
  - 'echo "  <name>fs.wasbs.impl</name>" >> /home/spark/hadoop-3.2.0/etc/hadoop/core-site.xml'
  - 'echo "  <value>org.apache.hadoop.fs.azure.NativeAzureFileSystem</value>" >> /home/spark/hadoop-3.2.0/etc/hadoop/core-site.xml'
  - 'echo "</property>" >> /home/spark/hadoop-3.2.0/etc/hadoop/core-site.xml'
  - 'echo "<property>" >> /home/spark/hadoop-3.2.0/etc/hadoop/core-site.xml'
  - 'echo "  <name>fs.AbstractFileSystem.wasbs.impl</name>" >> /home/spark/hadoop-3.2.0/etc/hadoop/core-site.xml'
  - 'echo "  <value>org.apache.hadoop.fs.azure.Wasbs</value>" >> /home/spark/hadoop-3.2.0/etc/hadoop/core-site.xml'
  - 'echo "</property>" >> /home/spark/hadoop-3.2.0/etc/hadoop/core-site.xml'
  - 'echo "</configuration>" >> /home/spark/hadoop-3.2.0/etc/hadoop/core-site.xml'
  - 'echo "<?xml version=\"1.0\" encoding=\"UTF-8\"?>" > /home/spark/hadoop-3.2.0/etc/hadoop/hdfs-site.xml'
  - 'echo "<configuration>" >> /home/spark/hadoop-3.2.0/etc/hadoop/hdfs-site.xml'
  - 'echo "  <property>" >> /home/spark/hadoop-3.2.0/etc/hadoop/hdfs-site.xml'
  - 'echo "    <name>dfs.namenode.datanode.registration.ip-hostname-check</name>" >> /home/spark/hadoop-3.2.0/etc/hadoop/hdfs-site.xml'
  - 'echo "    <value>false</value>" >> /home/spark/hadoop-3.2.0/etc/hadoop/hdfs-site.xml'
  - 'echo "  </property>" >> /home/spark/hadoop-3.2.0/etc/hadoop/hdfs-site.xml'  
  - 'echo "  <property>" >> /home/spark/hadoop-3.2.0/etc/hadoop/hdfs-site.xml'
  - 'echo "    <name>dfs.namenode.name.dir</name>" >> /home/spark/hadoop-3.2.0/etc/hadoop/hdfs-site.xml'
  - 'echo "    <value>/mnt/spark/namenode</value>" >> /home/spark/hadoop-3.2.0/etc/hadoop/hdfs-site.xml'
  - 'echo "  </property>" >> /home/spark/hadoop-3.2.0/etc/hadoop/hdfs-site.xml'
  - 'echo "  <property>" >> /home/spark/hadoop-3.2.0/etc/hadoop/hdfs-site.xml'
  - 'echo "    <name>dfs.datanode.data.dir</name>" >> /home/spark/hadoop-3.2.0/etc/hadoop/hdfs-site.xml'
  - 'echo "    <value>/mnt/spark/datanode</value>" >> /home/spark/hadoop-3.2.0/etc/hadoop/hdfs-site.xml'
  - 'echo "  </property>" >> /home/spark/hadoop-3.2.0/etc/hadoop/hdfs-site.xml'
  - 'echo "</configuration>" >> /home/spark/hadoop-3.2.0/etc/hadoop/hdfs-site.xml'
  - chown -R spark.spark /home/spark/hadoop-3.2.0
  - mkdir /mnt/spark/
  - chown spark.spark /mnt/spark
  - su -c "JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64/ /home/spark/spark-2.4.3-bin-without-hadoop/sbin/start-master.sh" spark
  - su -c "JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64/ /home/spark/hadoop-3.2.0/bin/hdfs namenode -format spark" spark
  - su -c "JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64/ /home/spark/hadoop-3.2.0/bin/hdfs --config /home/spark/hadoop-3.2.0/etc/hadoop/ --daemon start namenode" spark
